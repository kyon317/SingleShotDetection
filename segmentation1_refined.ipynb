{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sakshaymahna/cityscapes-depth-and-segmentation\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "cmd = f\"mv {path} ./\"\n",
    "\n",
    "os.system(cmd)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%mv ./1/data ./data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%rm -r ./1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x = np.load(\"./data/train/image/0.npy\")\n",
    "x = x*255\n",
    "x = x.astype(np.uint8)\n",
    "plt.imshow(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x = np.load(\"./data/train/label/0.npy\")\n",
    "plt.imshow(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from natsort import natsorted\n",
    "import os \n",
    "\n",
    "class CityScapes(Dataset):\n",
    "    def __init__(self, main_dir, transforms=None):\n",
    "        self.main_dir = main_dir\n",
    "        self.transforms = transforms\n",
    "        image_dir = os.path.join(main_dir, 'image')\n",
    "        self.images = []\n",
    "        for file in natsorted(os.listdir(image_dir)):\n",
    "            self.images.append(file)\n",
    "        label_dir = os.path.join(main_dir, 'label')\n",
    "        self.labels = []\n",
    "        for file in natsorted(os.listdir(image_dir)):\n",
    "            self.labels.append(file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.main_dir, 'image', self.images[index])\n",
    "        label_path = os.path.join(self.main_dir, 'label', self.images[index])\n",
    "        image, label = np.load(image_path), np.load(label_path)\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, mask=label)\n",
    "            image, label = transformed['image'], transformed['mask']\n",
    "            label = torch.where(label==13, 1, 0)\n",
    "        return image, label"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(),\n",
    "    A.Rotate(limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    ToTensorV2()\n",
    "])\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    ToTensorV2()\n",
    "])\n",
    "train_dataset = CityScapes(\"./data/train\", transforms=train_transforms)\n",
    "train_dataset, valid_dataset = random_split(train_dataset, (0.9, 0.1))\n",
    "test_dataset = CityScapes(\"./data/val\", transforms=test_transforms)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img, label = train_dataset[10]\n",
    "# plt.imshow(label)\n",
    "img = img.permute(1,2,0)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,10))\n",
    "axes[0].imshow(img.cpu().numpy())\n",
    "axes[1].imshow(label)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def simple_block(in_channel, out_channel):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channel),\n",
    "        nn.ReLU()\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "weight = torch.load(\"./vgg16_bn-6c64b313.pth\")\n",
    "for item in weight.items():\n",
    "    print(item[0], item[1].shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channel=3):\n",
    "        super().__init__()\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            simple_block(input_channel, 64),\n",
    "            simple_block(64, 64),\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            simple_block(64, 128),\n",
    "            simple_block(128, 128),\n",
    "        )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            simple_block(128, 256),\n",
    "            simple_block(256, 256),\n",
    "            simple_block(256, 256),\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            simple_block(256, 512),\n",
    "            simple_block(512, 512),\n",
    "            simple_block(512, 512),\n",
    "        )\n",
    "        self.encoder5 = nn.Sequential(\n",
    "            simple_block(512, 512),\n",
    "            simple_block(512, 512),\n",
    "            simple_block(512, 512),\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = self.encoder1(data)\n",
    "        x, id1 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
    "        x = self.encoder2(x)\n",
    "        x, id2 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
    "        x = self.encoder3(x)\n",
    "        x, id3 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
    "        x = self.encoder4(x)\n",
    "        x, id4 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
    "        x = self.encoder5(x)\n",
    "        x, id5 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        return x, [id1, id2, id3, id4, id5]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_channel=2):\n",
    "        super().__init__()\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            simple_block(512, 512),\n",
    "            simple_block(512, 512),\n",
    "            simple_block(512, 512),\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            simple_block(512, 512),\n",
    "            simple_block(512, 512),\n",
    "            simple_block(512, 256),\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            simple_block(256, 256),\n",
    "            simple_block(256, 256),\n",
    "            simple_block(256, 128),\n",
    "        )\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            simple_block(128, 128),\n",
    "            simple_block(128, 64),\n",
    "        )\n",
    "        self.decoder5 = nn.Sequential(\n",
    "            simple_block(64, 64),\n",
    "            simple_block(64, output_channel)\n",
    "        )\n",
    "    def forward(self, data, ids):\n",
    "        reverted_ids = ids[::-1]\n",
    "        x = F.max_unpool2d(data, indices=reverted_ids[0], kernel_size=2, stride=2)\n",
    "        x = self.decoder1(x)\n",
    "        x = F.max_unpool2d(x, indices=reverted_ids[1], kernel_size=2, stride=2)\n",
    "        x = self.decoder2(x)\n",
    "        x = F.max_unpool2d(x, indices=reverted_ids[2], kernel_size=2, stride=2)\n",
    "        x = self.decoder3(x)\n",
    "        x = F.max_unpool2d(x, indices=reverted_ids[3], kernel_size=2, stride=2)\n",
    "        x = self.decoder4(x)\n",
    "        x = F.max_unpool2d(x, indices=reverted_ids[4], kernel_size=2, stride=2)\n",
    "        x = self.decoder5(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!wget https://download.pytorch.org/models/vgg16_bn-6c64b313.pth"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SegNet(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.encoder = self.load_encoder(in_channel)\n",
    "        self.decoder = Decoder(out_channel)\n",
    "\n",
    "    def load_encoder(self, in_channel):\n",
    "        weight = torch.load(\"./vgg16_bn-6c64b313.pth\")\n",
    "        for key in list(weight.keys()):\n",
    "            if key.startswith('classifier'):\n",
    "                del weight[key]\n",
    "        encoder = Encoder(in_channel)\n",
    "\n",
    "        encoder_names = []\n",
    "        for key in encoder.state_dict().keys():\n",
    "            if 'num_batches_tracked' in key:\n",
    "                continue\n",
    "            encoder_names.append(key)\n",
    "        \n",
    "        new_weights = self.state_dict()\n",
    "        for key, value in zip(encoder_names, weight.values()):\n",
    "            new_weights[key] = value\n",
    "        \n",
    "        encoder.load_state_dict(new_weights)\n",
    "\n",
    "        return encoder\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, ids = self.encoder(data)\n",
    "        x = self.decoder(x, ids)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = SegNet(3, 2)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
